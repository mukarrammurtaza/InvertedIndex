{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caef64b2",
      "metadata": {
        "id": "caef64b2"
      },
      "outputs": [],
      "source": [
        "###FOR GOOGLE COLLAB\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# import io\n",
        "def wordList(fileName):\n",
        "  enc = 'iso-8859-15'\n",
        "  with open(fileName, 'r', encoding=enc) as f:\n",
        "    fileReading = f.read()\n",
        "  listOfWords=fileReading.split()\n",
        "  print(listOfWords)\n",
        "  return listOfWords   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e910e1ba",
      "metadata": {
        "id": "e910e1ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1baa75",
      "metadata": {
        "id": "4d1baa75"
      },
      "outputs": [],
      "source": [
        "#replace punctuation, replace /n, check if in stopwords, remove, handle case insensitivity\n",
        "\n",
        "def listToString(s):\n",
        "    str1 = \"\"\n",
        "    for ele in s:\n",
        "        str1 +=' '+ele\n",
        "    return str1\n",
        "\n",
        "def func(value):\n",
        "    return ''.join(value.splitlines())\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import words\n",
        "\n",
        "\n",
        "def removePuncs(wordList):#pre processing\n",
        "    nWordList=[]\n",
        "    newText=' '\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #ps = PorterStemmer()\n",
        "    text=listToString(wordList)\n",
        "    latest = text.translate(str.maketrans('', '', string.punctuation))    \n",
        "    latest=latest.lower()\n",
        "    tokens=word_tokenize(latest)\n",
        "    cleaned_tokens=[i for i in tokens if i not in stop_words]\n",
        "    \n",
        "    return cleaned_tokens\n",
        "   \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26db3192",
      "metadata": {
        "id": "26db3192"
      },
      "outputs": [],
      "source": [
        "\n",
        "def createVocabulary(wordList):# removing duplicates\n",
        "    dict={}\n",
        "    voc = [] # a list that will store words of the vocabulary\n",
        "    for word in wordList: #go through each word in the current doc\n",
        "        if word not in voc:\n",
        "            voc.append(word) #add word in corpus if not already added \n",
        "            #print(word)            \n",
        "    #print(dict\"\\n\")\n",
        "    return voc\n",
        "\n",
        "def termFrequencyInDoc(preprocessed):\n",
        "    voc=createVocabulary(preprocessed)\n",
        "    dict = {} #document frequency for every word in corpus\n",
        "    for word in voc:\n",
        "        k = 0 #initial document frequency set to 0\n",
        "        for term in preprocessed:\n",
        "            if word==term:        \n",
        "                k+=1 \n",
        "                dict[word] = k\n",
        " \n",
        "    return dict    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa45e04",
      "metadata": {
        "id": "caa45e04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f4de44",
      "metadata": {
        "id": "a2f4de44"
      },
      "outputs": [],
      "source": [
        "def wordDocFrequency(listOfDict):\n",
        "    \n",
        "    masterDict = {}\n",
        "    for dict in listOfDict:\n",
        "        masterDict.update(listOfDict[dict])\n",
        "    \n",
        "    \n",
        "    for key in masterDict:\n",
        "        dictCount = 0\n",
        "        for dict in listOfDict:\n",
        "            if key in listOfDict[dict].keys():\n",
        "                dictCount += 1\n",
        "                \n",
        "        masterDict[key] = dictCount\n",
        "    return masterDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99eeba6",
      "metadata": {
        "id": "d99eeba6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def inverseDocFrequency(dictList,base):\n",
        "    vocabulary = wordDocFrequency(dictList)\n",
        "    idf_corpus = {} #inverse_document frequency for every word in corpus\n",
        "    for word in vocabulary:\n",
        "        idf_corpus[word] = np.log2((base+1) / vocabulary[word]) #log_2 ((M+1)/k) i.e inverse document frequency\n",
        "    \n",
        "   \n",
        "            \n",
        "    return idf_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f33653f",
      "metadata": {
        "id": "1f33653f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tfidf(docList):\n",
        "  idf_corpus=[]\n",
        "  dictList = {}\n",
        "    \n",
        "  for doc_key in docList:\n",
        "      preprocessed=removePuncs(docList[doc_key])\n",
        "      dict=termFrequencyInDoc(preprocessed)\n",
        "      dictList[doc_key]=dict\n",
        "  idf_corpus=inverseDocFrequency(dictList, len(dictList))\n",
        "  for dict in dictList:\n",
        "      for key in dictList[dict]:\n",
        "          dictList[dict][key] = idf_corpus[key]\n",
        "  return dictList        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0983271d",
      "metadata": {
        "id": "0983271d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from os import listdir\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "file_list=os.listdir()\n",
        "file_list = [i for i in file_list if \".txt\" in i]\n",
        "print(file_list)\n",
        "docList={}\n",
        "for file in file_list:\n",
        "    doc=wordList(file)\n",
        "    docList[file]=doc# creating list of docs\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662aed7d",
      "metadata": {
        "id": "662aed7d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('words')\n",
        "fd=tfidf(docList)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2932375",
      "metadata": {
        "id": "b2932375"
      },
      "outputs": [],
      "source": [
        "def vectorSpaceModel(query,fd):\n",
        "\n",
        "  query_vocab = [] # will store the unique words that occur in the query\n",
        "  for word in query.split():\n",
        "    if word not in query_vocab:\n",
        "        query_vocab.append(word)\n",
        "  query_wc = {} # a dictionary to store count of a word in the query (i.e x_i according to lecture slides terminology)\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)      \n",
        "\n",
        "  relevance_scores = {} # a dictionary that will store the relevance score for each doc\n",
        "# doc_id will be the key and relevance score the value for this dictionary\n",
        "    \n",
        "  for doc_id in docList:\n",
        "    score = 0\n",
        "    #initialze the score for the doc to 0 at the start\n",
        "    for word in query_vocab:\n",
        "      if(word in fd[doc_id].keys()):\n",
        "        check=fd[doc_id][word]\n",
        "        q=query_wc[word]\n",
        "        score += query_wc[word] * fd[doc_id][word] # count of word in query * term_freq of the word\n",
        "    relevance_scores[doc_id] = score\n",
        "  #print(\"Document Relevancy Scores\\n\",relevance_scores)\n",
        "  sortedDict=sorted(relevance_scores, key=relevance_scores.get, reverse=True)\n",
        "  outputDict={}\n",
        "  for i in range(0,5):\n",
        "    outputDict[sortedDict[i]] =relevance_scores[sortedDict[i]]\n",
        "  return outputDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query1 = 'LDA'\n",
        "# query2 = 'Topic modeling'\n",
        "# query3 = 'Generative models'\n",
        "# query4 = 'Semantic relationships between terms'\n",
        "# query5 = 'Natural language Processing'\n",
        "# query6 = 'Text Mining'\n",
        "# query7 = 'Translation model'\n",
        "# query8 = 'Learning procedures for the lexicon'\n",
        "# query9 = 'Semantic evaluations'\n",
        "query10 = 'Information Retrieval'\n",
        "\n",
        "# Q1 = vectorSpaceModel(query1,fd)    \n",
        "# Q2 = vectorSpaceModel(query2,fd)    \n",
        "# Q3 = vectorSpaceModel(query3,fd)      \n",
        "# Q4 = vectorSpaceModel(query4,fd)    \n",
        "# Q5 = vectorSpaceModel(query5,fd)    \n",
        "# Q6 = vectorSpaceModel(query1,fd)    \n",
        "# Q7 = vectorSpaceModel(query2,fd)    \n",
        "# Q8 = vectorSpaceModel(query3,fd)      \n",
        "# Q9 = vectorSpaceModel(query4,fd)    \n",
        "Q10 = vectorSpaceModel(query5,fd)  \n",
        "# print('Top 5 Documents for Query 1: \\n', Q1)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 2: \\n', Q2)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 3: \\n', Q3)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 4: \\n', Q4)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 5: \\n', Q5)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 6: \\n', Q6)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 7: \\n', Q7)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 8: \\n', Q8)\n",
        "# print('\\n')\n",
        "# print('Top 5 Documents for Query 9: \\n', Q9)\n",
        "# print('\\n')\n",
        "print('Top 5 Documents for Query 10: \\n', Q10)"
      ],
      "metadata": {
        "id": "CCVbgcXf_s0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e652e4-61c0-4e93-921c-d7a1440e9471"
      },
      "id": "CCVbgcXf_s0c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Documents for Query 10: \n",
            " {'A97-1056.pdf.txt': 0.023382927437950687, 'D12-1090.pdf.txt': 0.023382927437950687, 'A88-1010.pdf.txt': 0.023382927437950687, 'D13-1080.pdf.txt': 0.023382927437950687, 'D11-1010.pdf.txt': 0.023382927437950687}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}